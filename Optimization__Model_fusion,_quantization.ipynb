{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Optimization_-Model-fusion-quantization/blob/main/Optimization__Model_fusion%2C_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54975c6",
      "metadata": {
        "id": "c54975c6"
      },
      "source": [
        "# Inference Optimization for Convolutional Netwroks\n",
        "### Part 1: Model fusion, quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a21dd26c",
      "metadata": {
        "id": "a21dd26c"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76b5ed4",
      "metadata": {
        "id": "a76b5ed4"
      },
      "source": [
        "### Notebook overview\n",
        "- Create CNN model and the quantized version of the same model\n",
        "- Compare difference in size and latency of two models\n",
        "- Fuse several blocks into one\n",
        "- Compare fused and quantized version with only fused version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a235c9f7",
      "metadata": {
        "id": "a235c9f7"
      },
      "source": [
        "### Create simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb803a99",
      "metadata": {
        "id": "fb803a99"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20,kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Convolutional  Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(in_features=50*53*53, out_features=500)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fully connected 2\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.Softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass the input through block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # pass the input through block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # flatten the output from the previous layer and pass it through fully connected 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # pass the input through fully connected 2 and Softmax\n",
        "        x = self.fc2(x)\n",
        "        output = self.Softmax(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "32dd9750",
      "metadata": {
        "id": "32dd9750"
      },
      "outputs": [],
      "source": [
        "# changes in network\n",
        "\n",
        "class NetQuant(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetQuant, self).__init__()\n",
        "        # Prepare for quanitzation\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20,kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(in_features=50*53*53, out_features=500)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fully connected 2\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.Softmax = nn.Softmax(1)\n",
        "\n",
        "        # Prepare for dequantization\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # pass the input through block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # pass the input through block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # flatten the output from the previous layer and pass it through fully connected 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # pass the input through fully connected 2 and Softmax\n",
        "        x = self.fc2(x)\n",
        "        x = self.dequant(x)\n",
        "        x = self.Softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a5c1f4d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c1f4d1",
        "outputId": "e5b21354-bfaf-4cb6-d949-4dc02fd0513c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetQuant(\n",
              "  (quant): QuantStub()\n",
              "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
              "  (Softmax): Softmax(dim=1)\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Define original and quantized models and prepae for evaluation\n",
        "\n",
        "net = Net()\n",
        "net.eval()\n",
        "net_quant = NetQuant()\n",
        "net_quant.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "77ca95c5",
      "metadata": {
        "id": "77ca95c5"
      },
      "outputs": [],
      "source": [
        "# Prepare model quantization and convert to quantized version\n",
        "net_quant.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "torch.backends.quantized.engine = \"fbgemm\"\n",
        "net_quant = torch.quantization.prepare(net_quant.cpu(), inplace=False)\n",
        "net_quant = torch.quantization.convert(net_quant, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07c2e55",
      "metadata": {
        "id": "b07c2e55"
      },
      "source": [
        "### Check size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "44c423f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44c423f6",
        "outputId": "9ff79280-a221-4781-ea44-606cda073934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size whitout quantization: 281 MB \n",
            " Size whit quantization: 70 MB\n",
            "Size ratio: 4.01\n"
          ]
        }
      ],
      "source": [
        "# Check model size\n",
        "def print_model_size(mdl):\n",
        "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
        "    size = round(os.path.getsize(\"tmp.pt\")/1e6)\n",
        "    os.remove('tmp.pt')\n",
        "    return size\n",
        "\n",
        "net_size = print_model_size(net)\n",
        "quant_size = print_model_size(net_quant)\n",
        "\n",
        "print(f'Size whitout quantization: {net_size} MB \\n Size whit quantization: {quant_size} MB')\n",
        "print(f'Size ratio: {round(net_size/quant_size, 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ce6cb3",
      "metadata": {
        "id": "a4ce6cb3"
      },
      "source": [
        "## Latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3c73a652",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c73a652",
        "outputId": "f37ff51f-9876-4b52-b1e8-bf21fd8fba3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floating point FP32\n",
            "1.46 s ± 169 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Quantized INT8\n",
            "921 ms ± 168 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "# input for the model\n",
        "inpp = torch.rand(32, 3, 224, 224)\n",
        "# compare the performance\n",
        "print(\"Floating point FP32\")\n",
        "%timeit net(inpp)\n",
        "\n",
        "print(\"Quantized INT8\")\n",
        "%timeit net_quant(inpp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61176f0",
      "metadata": {
        "id": "c61176f0"
      },
      "source": [
        "### Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8bdd7325",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdd7325",
        "outputId": "7956b5ab-bd91-47b1-98ff-7ba25ce20031"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetQuant(\n",
              "  (quant): QuantStub()\n",
              "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
              "  (Softmax): Softmax(dim=1)\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Define original and quantized models and prepae for evaluation\n",
        "\n",
        "net = Net()\n",
        "net.eval()\n",
        "net_quant = NetQuant()\n",
        "net_quant.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6ac0e44e",
      "metadata": {
        "id": "6ac0e44e"
      },
      "outputs": [],
      "source": [
        "# Perpare blocks for the fusion\n",
        "\n",
        "moduls_to_fuse =  [['conv1', 'relu1'],\n",
        "                   ['conv2', 'relu2'],\n",
        "                   ['fc1', 'relu3']]\n",
        "\n",
        "net_quant_fused = torch.quantization.fuse_modules(net_quant, moduls_to_fuse)\n",
        "\n",
        "net_fused = torch.quantization.fuse_modules(net, moduls_to_fuse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "12c1672f",
      "metadata": {
        "id": "12c1672f"
      },
      "outputs": [],
      "source": [
        "# Prepare and quantize the model\n",
        "\n",
        "net_quant_fused.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "torch.backends.quantized.engine = \"fbgemm\"\n",
        "net_quant_fused = torch.quantization.prepare(net_quant_fused.cpu(), inplace=False)\n",
        "net_quant_fused = torch.quantization.convert(net_quant_fused, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "521f4da1",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "521f4da1",
        "outputId": "00d167e8-a509-4f03-c264-d43afa9ff1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused and quantized model latency\n",
            "913 ms ± 173 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Fused model latency\n",
            "1.38 s ± 168 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "print(\"Fused and quantized model latency\")\n",
        "%timeit net_quant_fused(inpp)\n",
        "\n",
        "print(\"Fused model latency\")\n",
        "%timeit net_fused(inpp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}